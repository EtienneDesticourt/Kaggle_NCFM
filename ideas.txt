For the first submission I simply used the inceptionV3 model from keras with a tensorflow backend
and a random 80:20 train/validation split. It took 110 minutes to train.

For the second submission I used a subsample of the total training data to be able to train the model faster
and test hypotheses with the same method as submission 1. It took 10 minutes to train. I concluded that subsampling achieved its goal.

For the third submission I relabeled some images in the subsample. The LB loss increased from submission 2 even though the val
loss decreased. I couldn't conclude on the usefulness of relabeling. I concluded that I need to implement k-fold CV to have
a val-loss/LB-loss correspondance for future tests.

4th submission is a dud.

For the 5th submission I trained the inceptionV3 model with stratifield k-fold cross validation (k=5) on the original training set.

For the 6th submission I trained the inceptionV3 model with stratifield k-fold cross validation (k=5) on the relabelled training set (1.5% of images relabeled) the val loss went up but the lb score went down. I concluded that k-fold cross validation wasn't enough and that I needed to
split the training set in a meaninful way by separating sequences.

For the 7th submission I tried using masks over boats but it went meh.

For 8th and 9th I overfit and fucked it up.

For the 10th submission I just repeated the training process a bunch of times and averaged the prediction to make better use data augmentation.

11th to 22nd Trained 17 models with data augmentation and checked different combinations.

Cluster by boats
train model cluster truth
calc fish probability by boat and ensemble with original model


Remove duplicate/sequential images for faster training
Correctly relabel dataset
Get validation split by removing sequences
Data augmentation average
Feed into SVM
Split by Day/Night

#TODO:

Sort by day night
